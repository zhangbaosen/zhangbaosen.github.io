<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Paper on Efficient Reinforcement Learning Through Trajectory Generation</title>
  <meta name="description" content="A key barrier to using reinforcement learning (RL) in many real-world applications is the requirement of a large number of system interactions to learn a goo...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/2023/01/L4DC_trajectory">
  <link rel="alternate" type="application/rss+xml" title="Baosen Zhang" href="http://localhost:4000/feed.xml">
     <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>


  <body>

    <header class="site-header">

<script src="https://techblog.lankes.org/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<script language="JavaScript" type="text/javascript">
<!--
function toggleDiv(element){
 if(document.getElementById(element).style.display == 'none')
  document.getElementById(element).style.display = 'block';
 else
       document.getElementById(element).style.display = 'none';
}
//-->
</script>
  
  <div class="wrapper">

    <a class="site-title" href="/">Baosen Zhang</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
         <ul class="menu">

    
     <li><a href="/about/" class="page-link">About</a>
    
    </li>
    
    
     <li><a href="/research/" class="page-link">Research</a>
    
    </li>
    
    
    <li><a href="/teaching/" class="page-link">Teaching</a>
    <ul class="sub-menu">
    
    <li><a href="/teaching">EE 351 – Introduction to Energy Systems</a></li>
    
    <li><a href="/teaching/EE451">EE 451 - Wind Energy</a></li>
    
    <li><a href="/teaching/EE454">EE 454 - Power System Analysis</a></li>
    
    <li><a href="/teaching/">EE 554 – Large Scale Power Systems</a></li>
    
    <li><a href="/teaching/EE553">EE 553 - Power System Economics</a></li>
    
    <li><a href="/teaching/EE559">EE 559 – Optimization of Networks and Graphs</a></li>
    
    </ul>
    
    </li>
    
    
     <li><a href="/group/" class="page-link">Group</a>
    
    </li>
    
    </ul>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Paper on Efficient Reinforcement Learning Through Trajectory Generation</h1>
    <p class="post-meta"><time datetime="2023-01-19T00:00:00-05:00" itemprop="datePublished">Jan 19, 2023</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>A key barrier to using reinforcement learning (RL) in many real-world applications is the requirement of a large number of system interactions to learn a good control policy. Off-policy and Offline RL methods have been proposed to reduce the number of interactions with the physical environment by learning control policies from historical data. However, their performances suffer from the lack of exploration and the distributional shifts in trajectories once controllers are updated. Moreover, most RL methods require that all states are directly observed, which is difficult to be attained in many settings. To overcome these challenges, we propose a trajectory generation algorithm, which adaptively generates new trajectories as if the system is being operated and explored under the updated control policies. Motivated by the fundamental lemma for linear systems, assuming sufficient excitation, we generate trajectories from linear combinations of historical trajectories. For linear feedback control, we prove that the algorithm generates trajectories with the exact distribution as if they are sampled from the real system using the updated control policy. In particular, the algorithm extends to systems where the states are not directly observed. Experiments show that the proposed method significantly reduces the number of sampled data needed for RL algorithms.</p>
<ol class="bibliography"></ol>

  </div>

</article>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Baosen Zhang</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Baosen Zhang</li>
          <li><a href="mailto:zhangbao@uw.edu">zhangbao@uw.edu</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Baosen Zhang is an Associate Professor in the Electrical and Computer Engineering Department at the University of Washington.
</p>
      </div>
    </div>

  </div>

</footer>


   <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    
  </body>

</html>

